+ '[' 20 -le 20 ']'
+ local/chain/run_tdnn.sh
local/chain/run_tdnn.sh
local/nnet3/run_ivector_common.sh: preparing directory for low-resolution speed-perturbed data (for alignment)
utils/data/perturb_data_dir_speed_3way.sh: making sure the utt2dur and the reco2dur files are present
... in data/train_960_cleaned, because obtaining it after speed-perturbing
... would be very slow, and you might need them.
utils/data/get_utt2dur.sh: working out data/train_960_cleaned/utt2dur from data/train_960_cleaned/segments
utils/data/get_utt2dur.sh: computed data/train_960_cleaned/utt2dur
utils/data/get_reco2dur.sh: obtaining durations from recordings
utils/data/get_reco2dur.sh: could not get recording lengths from sphere-file headers, using wav-to-duration



utils/data/get_reco2dur.sh: computed data/train_960_cleaned/reco2dur
utils/data/perturb_data_dir_speed.sh: generated speed-perturbed version of data in data/train_960_cleaned, in data/train_960_cleaned_sp_speed0.9
utils/validate_data_dir.sh: Successfully validated data-directory data/train_960_cleaned_sp_speed0.9
utils/data/perturb_data_dir_speed.sh: generated speed-perturbed version of data in data/train_960_cleaned, in data/train_960_cleaned_sp_speed1.1
utils/validate_data_dir.sh: Successfully validated data-directory data/train_960_cleaned_sp_speed1.1
utils/data/combine_data.sh data/train_960_cleaned_sp data/train_960_cleaned data/train_960_cleaned_sp_speed0.9 data/train_960_cleaned_sp_speed1.1
utils/data/combine_data.sh: combined utt2uniq
utils/data/combine_data.sh: combined segments
utils/data/combine_data.sh: combined utt2spk
utils/data/combine_data.sh [info]: not combining utt2lang as it does not exist
utils/data/combine_data.sh: combined utt2dur
utils/data/combine_data.sh: combined reco2dur
utils/data/combine_data.sh [info]: **not combining feats.scp as it does not exist everywhere**
utils/data/combine_data.sh: combined text
utils/data/combine_data.sh [info]: **not combining cmvn.scp as it does not exist everywhere**
utils/data/combine_data.sh [info]: not combining vad.scp as it does not exist
utils/data/combine_data.sh [info]: not combining reco2file_and_channel as it does not exist
utils/data/combine_data.sh: combined wav.scp
utils/data/combine_data.sh: combined spk2gender
fix_data_dir.sh: kept all 904827 utterances.
fix_data_dir.sh: old files are kept in data/train_960_cleaned_sp/.backup
utils/data/perturb_data_dir_speed_3way.sh: generated 3-way speed-perturbed version of data in data/train_960_cleaned, in data/train_960_cleaned_sp
utils/validate_data_dir.sh: Successfully validated data-directory data/train_960_cleaned_sp
local/nnet3/run_ivector_common.sh: making MFCC features for low-resolution speed-perturbed data
steps/make_mfcc.sh --cmd queue.pl --mem 8G -q grid -V --nj 50 data/train_960_cleaned_sp
utils/validate_data_dir.sh: Successfully validated data-directory data/train_960_cleaned_sp
steps/make_mfcc.sh [info]: segments file exists: using that.
Succeeded creating MFCC features for train_960_cleaned_sp
steps/compute_cmvn_stats.sh data/train_960_cleaned_sp
Succeeded creating CMVN stats for train_960_cleaned_sp
local/nnet3/run_ivector_common.sh: fixing input data-dir to remove nonexistent features, in case some
.. speed-perturbed segments were too short.
fix_data_dir.sh: kept all 904827 utterances.
fix_data_dir.sh: old files are kept in data/train_960_cleaned_sp/.backup
local/nnet3/run_ivector_common.sh: aligning with the perturbed low-resolution data
steps/align_fmllr.sh --nj 100 --cmd queue.pl --mem 8G -q grid -V data/train_960_cleaned_sp data/lang exp/tri6b_cleaned exp/tri6b_cleaned_ali_train_960_cleaned_sp
steps/align_fmllr.sh: feature type is lda
steps/align_fmllr.sh: compiling training graphs
steps/align_fmllr.sh: aligning data in data/train_960_cleaned_sp using exp/tri6b_cleaned/final.alimdl and speaker-independent features.
steps/align_fmllr.sh: computing fMLLR transforms
steps/align_fmllr.sh: doing final alignment.
steps/align_fmllr.sh: done aligning data.
steps/diagnostic/analyze_alignments.sh --cmd queue.pl --mem 8G -q grid -V data/lang exp/tri6b_cleaned_ali_train_960_cleaned_sp
steps/diagnostic/analyze_alignments.sh: see stats in exp/tri6b_cleaned_ali_train_960_cleaned_sp/log/analyze_alignments.log
1124 warnings in exp/tri6b_cleaned_ali_train_960_cleaned_sp/log/align_pass1.*.log
695 warnings in exp/tri6b_cleaned_ali_train_960_cleaned_sp/log/align_pass2.*.log
8 warnings in exp/tri6b_cleaned_ali_train_960_cleaned_sp/log/fmllr.*.log
local/nnet3/run_ivector_common.sh: creating high-resolution MFCC features
utils/copy_data_dir.sh: copied data from data/train_960_cleaned_sp to data/train_960_cleaned_sp_hires
utils/validate_data_dir.sh: Successfully validated data-directory data/train_960_cleaned_sp_hires
utils/copy_data_dir.sh: copied data from data/test_clean to data/test_clean_hires
utils/validate_data_dir.sh: Successfully validated data-directory data/test_clean_hires
utils/copy_data_dir.sh: copied data from data/test_other to data/test_other_hires
utils/validate_data_dir.sh: Successfully validated data-directory data/test_other_hires
utils/copy_data_dir.sh: copied data from data/dev_clean to data/dev_clean_hires
utils/validate_data_dir.sh: Successfully validated data-directory data/dev_clean_hires
utils/copy_data_dir.sh: copied data from data/dev_other to data/dev_other_hires
utils/validate_data_dir.sh: Successfully validated data-directory data/dev_other_hires
utils/data/perturb_data_dir_volume.sh: data/train_960_cleaned_sp_hires/feats.scp exists; moving it to data/train_960_cleaned_sp_hires/.backup/ as it wouldn't be valid any more.
utils/data/perturb_data_dir_volume.sh: added volume perturbation to the data in data/train_960_cleaned_sp_hires
steps/make_mfcc.sh --nj 70 --mfcc-config conf/mfcc_hires.conf --cmd queue.pl --mem 8G -q grid -V data/train_960_cleaned_sp_hires
utils/validate_data_dir.sh: Successfully validated data-directory data/train_960_cleaned_sp_hires
steps/make_mfcc.sh [info]: segments file exists: using that.
Succeeded creating MFCC features for train_960_cleaned_sp_hires
steps/compute_cmvn_stats.sh data/train_960_cleaned_sp_hires
Succeeded creating CMVN stats for train_960_cleaned_sp_hires
fix_data_dir.sh: kept all 904827 utterances.
fix_data_dir.sh: old files are kept in data/train_960_cleaned_sp_hires/.backup
steps/make_mfcc.sh --nj 70 --mfcc-config conf/mfcc_hires.conf --cmd queue.pl --mem 8G -q grid -V data/test_clean_hires
steps/make_mfcc.sh: moving data/test_clean_hires/feats.scp to data/test_clean_hires/.backup
utils/validate_data_dir.sh: Successfully validated data-directory data/test_clean_hires
steps/make_mfcc.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
Succeeded creating MFCC features for test_clean_hires
steps/compute_cmvn_stats.sh data/test_clean_hires
Succeeded creating CMVN stats for test_clean_hires
fix_data_dir.sh: kept all 2620 utterances.
fix_data_dir.sh: old files are kept in data/test_clean_hires/.backup
steps/make_mfcc.sh --nj 70 --mfcc-config conf/mfcc_hires.conf --cmd queue.pl --mem 8G -q grid -V data/test_other_hires
steps/make_mfcc.sh: moving data/test_other_hires/feats.scp to data/test_other_hires/.backup
utils/validate_data_dir.sh: Successfully validated data-directory data/test_other_hires
steps/make_mfcc.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
Succeeded creating MFCC features for test_other_hires
steps/compute_cmvn_stats.sh data/test_other_hires
Succeeded creating CMVN stats for test_other_hires
fix_data_dir.sh: kept all 2939 utterances.
fix_data_dir.sh: old files are kept in data/test_other_hires/.backup
steps/make_mfcc.sh --nj 70 --mfcc-config conf/mfcc_hires.conf --cmd queue.pl --mem 8G -q grid -V data/dev_clean_hires
steps/make_mfcc.sh: moving data/dev_clean_hires/feats.scp to data/dev_clean_hires/.backup
utils/validate_data_dir.sh: Successfully validated data-directory data/dev_clean_hires
steps/make_mfcc.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
Succeeded creating MFCC features for dev_clean_hires
steps/compute_cmvn_stats.sh data/dev_clean_hires
Succeeded creating CMVN stats for dev_clean_hires
fix_data_dir.sh: kept all 2703 utterances.
fix_data_dir.sh: old files are kept in data/dev_clean_hires/.backup
steps/make_mfcc.sh --nj 70 --mfcc-config conf/mfcc_hires.conf --cmd queue.pl --mem 8G -q grid -V data/dev_other_hires
steps/make_mfcc.sh: moving data/dev_other_hires/feats.scp to data/dev_other_hires/.backup
utils/validate_data_dir.sh: Successfully validated data-directory data/dev_other_hires
steps/make_mfcc.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
Succeeded creating MFCC features for dev_other_hires
steps/compute_cmvn_stats.sh data/dev_other_hires
Succeeded creating CMVN stats for dev_other_hires
fix_data_dir.sh: kept all 2864 utterances.
fix_data_dir.sh: old files are kept in data/dev_other_hires/.backup
utils/subset_data_dir.sh: reducing #utt from 904827 to 60000
local/nnet3/run_ivector_common.sh: making a subset of data to train the diagonal UBM and the PCA transform.
utils/data/subset_data_dir.sh: reducing #utt from 904827 to 9048
local/nnet3/run_ivector_common.sh: computing a PCA transform from the hires data.
steps/online/nnet2/get_pca_transform.sh --cmd queue.pl --mem 8G -q grid -V --splice-opts --left-context=3 --right-context=3 --max-utts 10000 --subsample 2 exp/nnet3_cleaned/diag_ubm/train_960_cleaned_sp_hires_subset exp/nnet3_cleaned/pca_transform
Done estimating PCA transform in exp/nnet3_cleaned/pca_transform
local/nnet3/run_ivector_common.sh: training the diagonal UBM.
steps/online/nnet2/train_diag_ubm.sh --cmd queue.pl --mem 8G -q grid -V --nj 30 --num-frames 700000 --num-threads 6 exp/nnet3_cleaned/diag_ubm/train_960_cleaned_sp_hires_subset 512 exp/nnet3_cleaned/pca_transform exp/nnet3_cleaned/diag_ubm
steps/online/nnet2/train_diag_ubm.sh: Directory exp/nnet3_cleaned/diag_ubm already exists. Backing up diagonal UBM in exp/nnet3_cleaned/diag_ubm/backup.UTI
steps/online/nnet2/train_diag_ubm.sh: initializing model from E-M in memory,
steps/online/nnet2/train_diag_ubm.sh: starting from 256 Gaussians, reaching 512;
steps/online/nnet2/train_diag_ubm.sh: for 20 iterations, using at most 700000 frames of data
Getting Gaussian-selection info
steps/online/nnet2/train_diag_ubm.sh: will train for 4 iterations, in parallel over
steps/online/nnet2/train_diag_ubm.sh: 30 machines, parallelized with 'queue.pl --mem 8G -q grid -V'
steps/online/nnet2/train_diag_ubm.sh: Training pass 0
steps/online/nnet2/train_diag_ubm.sh: Training pass 1
steps/online/nnet2/train_diag_ubm.sh: Training pass 2
steps/online/nnet2/train_diag_ubm.sh: Training pass 3
local/nnet3/run_ivector_common.sh: training the iVector extractor
steps/online/nnet2/train_ivector_extractor.sh --cmd queue.pl --mem 8G -q grid -V --nj 10 --num-processes 3 data/train_960_cleaned_sp_hires_60k exp/nnet3_cleaned/diag_ubm exp/nnet3_cleaned/extractor
steps/online/nnet2/train_ivector_extractor.sh: doing Gaussian selection and posterior computation
Accumulating stats (pass 0)
Summing accs (pass 0)
Updating model (pass 0)
Accumulating stats (pass 1)
Summing accs (pass 1)
Updating model (pass 1)
Accumulating stats (pass 2)
Summing accs (pass 2)
Updating model (pass 2)
Accumulating stats (pass 3)
Summing accs (pass 3)
Updating model (pass 3)
Accumulating stats (pass 4)
Summing accs (pass 4)
Updating model (pass 4)
Accumulating stats (pass 5)
Summing accs (pass 5)
Updating model (pass 5)
Accumulating stats (pass 6)
Summing accs (pass 6)
Updating model (pass 6)
Accumulating stats (pass 7)
Summing accs (pass 7)
Updating model (pass 7)
Accumulating stats (pass 8)
Summing accs (pass 8)
Updating model (pass 8)
Accumulating stats (pass 9)
Summing accs (pass 9)
Updating model (pass 9)
local/nnet3/run_ivector_common.sh: extracting iVectors for training data
utils/data/modify_speaker_info.sh: copied data from data/train_960_cleaned_sp_hires to exp/nnet3_cleaned/ivectors_train_960_cleaned_sp_hires/train_960_cleaned_sp_hires_max2, number of speakers changed from 16398 to 456441
utils/validate_data_dir.sh: Successfully validated data-directory exp/nnet3_cleaned/ivectors_train_960_cleaned_sp_hires/train_960_cleaned_sp_hires_max2
steps/online/nnet2/extract_ivectors_online.sh --cmd queue.pl --mem 8G -q grid -V --nj 60 exp/nnet3_cleaned/ivectors_train_960_cleaned_sp_hires/train_960_cleaned_sp_hires_max2 exp/nnet3_cleaned/extractor exp/nnet3_cleaned/ivectors_train_960_cleaned_sp_hires
filter_scps.pl: warning: some input lines were output to multiple files [OK if splitting per utt]
steps/online/nnet2/extract_ivectors_online.sh: extracting iVectors

steps/online/nnet2/extract_ivectors_online.sh: combining iVectors across jobs
steps/online/nnet2/extract_ivectors_online.sh: done extracting (online) iVectors to exp/nnet3_cleaned/ivectors_train_960_cleaned_sp_hires using the extractor in exp/nnet3_cleaned/extractor.
local/nnet3/run_ivector_common.sh: extracting iVectors for dev and test data
steps/online/nnet2/extract_ivectors_online.sh --cmd queue.pl --mem 8G -q grid -V --nj 20 data/test_clean_hires exp/nnet3_cleaned/extractor exp/nnet3_cleaned/ivectors_test_clean_hires
steps/online/nnet2/extract_ivectors_online.sh: extracting iVectors
steps/online/nnet2/extract_ivectors_online.sh: combining iVectors across jobs
steps/online/nnet2/extract_ivectors_online.sh: done extracting (online) iVectors to exp/nnet3_cleaned/ivectors_test_clean_hires using the extractor in exp/nnet3_cleaned/extractor.
steps/online/nnet2/extract_ivectors_online.sh --cmd queue.pl --mem 8G -q grid -V --nj 20 data/test_other_hires exp/nnet3_cleaned/extractor exp/nnet3_cleaned/ivectors_test_other_hires
steps/online/nnet2/extract_ivectors_online.sh: extracting iVectors
steps/online/nnet2/extract_ivectors_online.sh: combining iVectors across jobs
steps/online/nnet2/extract_ivectors_online.sh: done extracting (online) iVectors to exp/nnet3_cleaned/ivectors_test_other_hires using the extractor in exp/nnet3_cleaned/extractor.
steps/online/nnet2/extract_ivectors_online.sh --cmd queue.pl --mem 8G -q grid -V --nj 20 data/dev_clean_hires exp/nnet3_cleaned/extractor exp/nnet3_cleaned/ivectors_dev_clean_hires
steps/online/nnet2/extract_ivectors_online.sh: extracting iVectors
steps/online/nnet2/extract_ivectors_online.sh: combining iVectors across jobs
steps/online/nnet2/extract_ivectors_online.sh: done extracting (online) iVectors to exp/nnet3_cleaned/ivectors_dev_clean_hires using the extractor in exp/nnet3_cleaned/extractor.
steps/online/nnet2/extract_ivectors_online.sh --cmd queue.pl --mem 8G -q grid -V --nj 20 data/dev_other_hires exp/nnet3_cleaned/extractor exp/nnet3_cleaned/ivectors_dev_other_hires
steps/online/nnet2/extract_ivectors_online.sh: extracting iVectors
steps/online/nnet2/extract_ivectors_online.sh: combining iVectors across jobs
steps/online/nnet2/extract_ivectors_online.sh: done extracting (online) iVectors to exp/nnet3_cleaned/ivectors_dev_other_hires using the extractor in exp/nnet3_cleaned/extractor.
local/chain/run_chain_common.sh --stage 0 --gmm-dir exp/tri6b_cleaned --ali-dir exp/tri6b_cleaned_ali_train_960_cleaned_sp --lores-train-data-dir data/train_960_cleaned_sp --lang data/lang_chain --lat-dir exp/chain_cleaned/tri6b_cleaned_train_960_cleaned_sp_lats --num-leaves 7000 --tree-dir exp/chain_cleaned/tree_sp
local/chain/run_chain_common.sh: creating lang directory with one state per phone.
steps/align_fmllr_lats.sh --nj 100 --cmd queue.pl --mem 8G -q grid -V data/train_960_cleaned_sp data/lang_chain exp/tri6b_cleaned exp/chain_cleaned/tri6b_cleaned_train_960_cleaned_sp_lats
steps/align_fmllr_lats.sh: feature type is lda
steps/align_fmllr_lats.sh: compiling training graphs
steps/align_fmllr_lats.sh: aligning data in data/train_960_cleaned_sp using exp/tri6b_cleaned/final.alimdl and speaker-independent features.
steps/align_fmllr_lats.sh: computing fMLLR transforms
steps/align_fmllr_lats.sh: generating lattices containing alternate pronunciations.
steps/align_fmllr_lats.sh: done generating lattices from training transcripts.
1159 warnings in exp/chain_cleaned/tri6b_cleaned_train_960_cleaned_sp_lats/log/align_pass1.*.log
100 warnings in exp/chain_cleaned/tri6b_cleaned_train_960_cleaned_sp_lats/log/generate_lattices.*.log
9 warnings in exp/chain_cleaned/tri6b_cleaned_train_960_cleaned_sp_lats/log/fmllr.*.log
steps/nnet3/chain/build_tree.sh --frame-subsampling-factor 3 --context-opts --context-width=2 --central-position=1 --cmd queue.pl --mem 8G -q grid -V 7000 data/train_960_cleaned_sp data/lang_chain exp/tri6b_cleaned_ali_train_960_cleaned_sp exp/chain_cleaned/tree_sp
steps/nnet3/chain/build_tree.sh: feature type is lda
steps/nnet3/chain/build_tree.sh: Using transforms from exp/tri6b_cleaned_ali_train_960_cleaned_sp
steps/nnet3/chain/build_tree.sh: Initializing monophone model (for alignment conversion, in case topology changed)
steps/nnet3/chain/build_tree.sh: Accumulating tree stats
steps/nnet3/chain/build_tree.sh: Getting questions for tree clustering.
steps/nnet3/chain/build_tree.sh: Building the tree
steps/nnet3/chain/build_tree.sh: Initializing the model
steps/nnet3/chain/build_tree.sh: Converting alignments from exp/tri6b_cleaned_ali_train_960_cleaned_sp to use current tree
steps/nnet3/chain/build_tree.sh: Done building tree
local/chain/run_tdnn.sh: creating neural net configs using the xconfig parser
tree-info exp/chain_cleaned/tree_sp/tree
steps/nnet3/xconfig_to_configs.py --xconfig-file exp/chain_cleaned/tdnn_1d_sp/configs/network.xconfig --config-dir exp/chain_cleaned/tdnn_1d_sp/configs/
nnet3-init exp/chain_cleaned/tdnn_1d_sp/configs//init.config exp/chain_cleaned/tdnn_1d_sp/configs//init.raw
LOG (nnet3-init[5.5.76~2-535b]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/chain_cleaned/tdnn_1d_sp/configs//init.raw
nnet3-info exp/chain_cleaned/tdnn_1d_sp/configs//init.raw
nnet3-init exp/chain_cleaned/tdnn_1d_sp/configs//ref.config exp/chain_cleaned/tdnn_1d_sp/configs//ref.raw
LOG (nnet3-init[5.5.76~2-535b]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/chain_cleaned/tdnn_1d_sp/configs//ref.raw
nnet3-info exp/chain_cleaned/tdnn_1d_sp/configs//ref.raw
nnet3-init exp/chain_cleaned/tdnn_1d_sp/configs//ref.config exp/chain_cleaned/tdnn_1d_sp/configs//ref.raw
LOG (nnet3-init[5.5.76~2-535b]:main():nnet3-init.cc:80) Initialized raw neural net and wrote it to exp/chain_cleaned/tdnn_1d_sp/configs//ref.raw
nnet3-info exp/chain_cleaned/tdnn_1d_sp/configs//ref.raw
2019-08-29 11:18:35,697 [steps/nnet3/chain/train.py:33 - <module> - INFO ] Starting chain model trainer (train.py)
steps/nnet3/chain/train.py --stage -10 --cmd queue.pl --mem 8G -V --feat.online-ivector-dir exp/nnet3_cleaned/ivectors_train_960_cleaned_sp_hires --feat.cmvn-opts --norm-means=false --norm-vars=false --chain.xent-regularize 0.1 --chain.leaky-hmm-coefficient 0.1 --chain.l2-regularize 0.0 --chain.apply-deriv-weights false --chain.lm-opts=--num-extra-lm-states=2000 --egs.dir  --egs.stage -10 --egs.opts --frames-overlap-per-eg 0 --constrained false --egs.chunk-width 150,110,100 --trainer.dropout-schedule 0,0@0.20,0.5@0.50,0 --trainer.add-option=--optimization.memory-compression-level=2 --trainer.num-chunk-per-minibatch 64 --trainer.frames-per-iter 2500000 --trainer.num-epochs 4 --trainer.optimization.num-jobs-initial 3 --trainer.optimization.num-jobs-final 16 --trainer.optimization.initial-effective-lrate 0.00015 --trainer.optimization.final-effective-lrate 0.000015 --trainer.max-param-change 2.0 --cleanup.remove-egs true --feat-dir data/train_960_cleaned_sp_hires --tree-dir exp/chain_cleaned/tree_sp --lat-dir exp/chain_cleaned/tri6b_cleaned_train_960_cleaned_sp_lats --dir exp/chain_cleaned/tdnn_1d_sp
['steps/nnet3/chain/train.py', '--stage', '-10', '--cmd', 'queue.pl --mem 8G -V', '--feat.online-ivector-dir', 'exp/nnet3_cleaned/ivectors_train_960_cleaned_sp_hires', '--feat.cmvn-opts', '--norm-means=false --norm-vars=false', '--chain.xent-regularize', '0.1', '--chain.leaky-hmm-coefficient', '0.1', '--chain.l2-regularize', '0.0', '--chain.apply-deriv-weights', 'false', '--chain.lm-opts=--num-extra-lm-states=2000', '--egs.dir', '', '--egs.stage', '-10', '--egs.opts', '--frames-overlap-per-eg 0 --constrained false', '--egs.chunk-width', '150,110,100', '--trainer.dropout-schedule', '0,0@0.20,0.5@0.50,0', '--trainer.add-option=--optimization.memory-compression-level=2', '--trainer.num-chunk-per-minibatch', '64', '--trainer.frames-per-iter', '2500000', '--trainer.num-epochs', '4', '--trainer.optimization.num-jobs-initial', '3', '--trainer.optimization.num-jobs-final', '16', '--trainer.optimization.initial-effective-lrate', '0.00015', '--trainer.optimization.final-effective-lrate', '0.000015', '--trainer.max-param-change', '2.0', '--cleanup.remove-egs', 'true', '--feat-dir', 'data/train_960_cleaned_sp_hires', '--tree-dir', 'exp/chain_cleaned/tree_sp', '--lat-dir', 'exp/chain_cleaned/tri6b_cleaned_train_960_cleaned_sp_lats', '--dir', 'exp/chain_cleaned/tdnn_1d_sp']
2019-08-29 11:18:35,931 [steps/nnet3/chain/train.py:237 - process_args - WARNING ] You are running with one thread but you have not compiled
                   for CUDA.  You may be running a setup optimized for GPUs.
                   If you have GPUs and have nvcc installed, go to src/ and do
                   ./configure; make
2019-08-29 11:18:35,945 [steps/nnet3/chain/train.py:271 - train - INFO ] Arguments for the experiment
{'alignment_subsampling_factor': 3,
 'apply_deriv_weights': False,
 'backstitch_training_interval': 1,
 'backstitch_training_scale': 0.0,
 'chunk_left_context': 0,
 'chunk_left_context_initial': -1,
 'chunk_right_context': 0,
 'chunk_right_context_final': -1,
 'chunk_width': '150,110,100',
 'cleanup': True,
 'cmvn_opts': '--norm-means=false --norm-vars=false',
 'combine_sum_to_one_penalty': 0.0,
 'command': 'queue.pl --mem 8G -V',
 'compute_per_dim_accuracy': False,
 'deriv_truncate_margin': None,
 'dir': 'exp/chain_cleaned/tdnn_1d_sp',
 'do_final_combination': True,
 'dropout_schedule': '0,0@0.20,0.5@0.50,0',
 'egs_command': None,
 'egs_dir': None,
 'egs_opts': '--frames-overlap-per-eg 0 --constrained false',
 'egs_stage': -10,
 'email': None,
 'exit_stage': None,
 'feat_dir': 'data/train_960_cleaned_sp_hires',
 'final_effective_lrate': 1.5e-05,
 'frame_subsampling_factor': 3,
 'frames_per_iter': 2500000,
 'initial_effective_lrate': 0.00015,
 'input_model': None,
 'l2_regularize': 0.0,
 'lat_dir': 'exp/chain_cleaned/tri6b_cleaned_train_960_cleaned_sp_lats',
 'leaky_hmm_coefficient': 0.1,
 'left_deriv_truncate': None,
 'left_tolerance': 5,
 'lm_opts': '--num-extra-lm-states=2000',
 'max_lda_jobs': 10,
 'max_models_combine': 20,
 'max_objective_evaluations': 30,
 'max_param_change': 2.0,
 'momentum': 0.0,
 'num_chunk_per_minibatch': '64',
 'num_epochs': 4.0,
 'num_jobs_final': 16,
 'num_jobs_initial': 3,
 'online_ivector_dir': 'exp/nnet3_cleaned/ivectors_train_960_cleaned_sp_hires',
 'preserve_model_interval': 100,
 'presoftmax_prior_scale_power': -0.25,
 'proportional_shrink': 0.0,
 'rand_prune': 4.0,
 'remove_egs': True,
 'reporting_interval': 0.1,
 'right_tolerance': 5,
 'samples_per_iter': 400000,
 'shrink_saturation_threshold': 0.4,
 'shrink_value': 1.0,
 'shuffle_buffer_size': 5000,
 'srand': 0,
 'stage': -10,
 'train_opts': ['--optimization.memory-compression-level=2'],
 'tree_dir': 'exp/chain_cleaned/tree_sp',
 'use_gpu': 'yes',
 'xent_regularize': 0.1}
2019-08-29 11:22:26,629 [steps/nnet3/chain/train.py:325 - train - INFO ] Creating phone language-model
2019-08-29 11:24:47,832 [steps/nnet3/chain/train.py:330 - train - INFO ] Creating denominator FST
copy-transition-model exp/chain_cleaned/tree_sp/final.mdl exp/chain_cleaned/tdnn_1d_sp/0.trans_mdl
LOG (copy-transition-model[5.5.76~2-535b]:main():copy-transition-model.cc:62) Copied transition model.
2019-08-29 11:25:00,194 [steps/nnet3/chain/train.py:337 - train - INFO ] Initializing a basic network for estimating preconditioning matrix
2019-08-29 11:25:10,974 [steps/nnet3/chain/train.py:359 - train - INFO ] Generating egs
steps/nnet3/chain/get_egs.sh --frames-overlap-per-eg 0 --constrained false --cmd queue.pl --mem 8G -V --cmvn-opts --norm-means=false --norm-vars=false --online-ivector-dir exp/nnet3_cleaned/ivectors_train_960_cleaned_sp_hires --left-context 41 --right-context 41 --left-context-initial -1 --right-context-final -1 --left-tolerance 5 --right-tolerance 5 --frame-subsampling-factor 3 --alignment-subsampling-factor 3 --stage -10 --frames-per-iter 2500000 --frames-per-eg 150,110,100 --srand 0 data/train_960_cleaned_sp_hires exp/chain_cleaned/tdnn_1d_sp exp/chain_cleaned/tri6b_cleaned_train_960_cleaned_sp_lats exp/chain_cleaned/tdnn_1d_sp/egs
feat-to-len 'scp:head -n 10 data/train_960_cleaned_sp_hires/feats.scp|' ark,t:-
File data/train_960_cleaned_sp_hires/utt2uniq exists, so augmenting valid_uttlist to
include all perturbed versions of the same 'real' utterances.
steps/nnet3/chain/get_egs.sh: creating egs.  To ensure they are not deleted later you can do:  touch exp/chain_cleaned/tdnn_1d_sp/egs/.nodelete
steps/nnet3/chain/get_egs.sh: feature type is raw
tree-info exp/chain_cleaned/tdnn_1d_sp/tree
feat-to-dim scp:exp/nnet3_cleaned/ivectors_train_960_cleaned_sp_hires/ivector_online.scp -
steps/nnet3/chain/get_egs.sh: working out number of frames of training data
steps/nnet3/chain/get_egs.sh: working out feature dim
steps/nnet3/chain/get_egs.sh: creating 413 archives, each with 24999 egs, with
steps/nnet3/chain/get_egs.sh:   150,110,100 labels per example, and (left,right) context = (41,41)
steps/nnet3/chain/get_egs.sh: Getting validation and training subset examples in background.
steps/nnet3/chain/get_egs.sh: Generating training examples on disk
... Getting subsets of validation examples for diagnostics and combination.
steps/nnet3/chain/get_egs.sh: recombining and shuffling order of archives on disk
steps/nnet3/chain/get_egs.sh: removing temporary archives
steps/nnet3/chain/get_egs.sh: removing temporary alignments, lattices and transforms
steps/nnet3/chain/get_egs.sh: Finished preparing training examples
2019-08-29 11:59:05,359 [steps/nnet3/chain/train.py:408 - train - INFO ] Copying the properties from exp/chain_cleaned/tdnn_1d_sp/egs to exp/chain_cleaned/tdnn_1d_sp
2019-08-29 11:59:05,403 [steps/nnet3/chain/train.py:422 - train - INFO ] Computing the preconditioning matrix for input features
2019-08-29 12:00:07,117 [steps/nnet3/chain/train.py:431 - train - INFO ] Preparing the initial acoustic model.
2019-08-29 12:00:39,368 [steps/nnet3/chain/train.py:465 - train - INFO ] Training will run for 4.0 epochs = 521 iterations
2019-08-29 12:00:39,392 [steps/nnet3/chain/train.py:507 - train - INFO ] Iter: 0/520    Epoch: 0.00/4.0 (0.0% complete)    lr: 0.000450


